# LinkedIn Scraper - Product Requirements Document

## Overview
LinkedIn Scraper is an automated **backend system** for lead generation designed for sales and marketing teams. It enables businesses to discover potential leads by automating LinkedIn searches, profile data scraping, and lead management via a **RESTful API**. The system helps streamline the prospecting process for B2B sales teams by eliminating manual searches and data entry, allowing them to focus on qualifying and converting leads into customers using a separate frontend application.

## Core Features

### 1. User Management & Authentication
- Role-based authentication system supporting admin and standard user roles via a secure API.
- JWT-based security with bcrypt password hashing.
- API endpoints for user profile management and access control.
- Internal logic for rating scraped profiles (represented as 'Client' users) based on quality metrics.

### 2. LinkedIn Account Management
- **Secure API endpoints** for managing LinkedIn credentials used for scraping.
- CRUD operations for multiple LinkedIn accounts.
- **Critical Security Note:** Requires implementation of encryption-at-rest for stored passwords.
- Utilization of accounts in campaigns with proper rotation.

### 3. Proxy Management
- API endpoints for managing proxy server addresses.
- Support for authenticated and non-authenticated proxies.
- Integration with Selenium browser instances for IP rotation.

### 4. Campaign Management
- API endpoints for creating and managing search campaigns with customizable parameters.
- Support for search queries with filters (company, school, past company).
- Scheduling capabilities for automated execution via backend cron job.
- API endpoints for status tracking and result monitoring.

### 5. Automated LinkedIn Search
- Headless browser automation using Selenium WebDriver (platform-specific chromedriver).
- Intelligent navigation through LinkedIn search results via backend process.
- Implementation of anti-detection measures (random delays, proxy rotation).
- API-driven handling of login challenges (CAPTCHA, OTP verification) requiring user interaction via the consuming application.

### 6. Profile Data Scraping
- Backend process for extracting detailed profile information (name, title, experience, education, etc.).
- De-duplication logic using a dedicated collection (`PreviousLeads`).
- Structured storage of profile data within the `User` model (role: CLIENT).

### 7. Lead Management
- API endpoints for organizing and managing scraped profiles (linked via `Lead` model).
- API endpoints for assigning leads to users for follow-up.
- API endpoints for status tracking and filtering capabilities.
- Management of custom statuses for lead qualification workflow.

### 8. Lead Annotations
- API endpoints for adding/managing comments on leads.
- Automated activity logging for lead interactions and scraping events.
- API endpoints for retrieving lead comments and logs.

### 9. Email Integration
- API endpoints for configuring email settings (SMTP).
- Automated alerts for critical system events (e.g., login failures).
- Optional API endpoint for triggering campaign data export via email.

## User Experience

**Note:** The User Experience described below pertains to the **consuming application (separate project)** that interacts with this backend API.

### User Personas

1.  **Admin User (via Frontend)**
    *   Manages system settings, users, LinkedIn accounts, and proxies through the UI.
    *   Monitors system performance via API data.
    *   Controls access and permissions.

2.  **Standard User (via Frontend)**
    *   Creates and manages campaigns through the UI.
    *   Reviews and processes leads displayed in the UI.
    *   Assigns leads and tracks status via UI interactions calling the API.

3.  **Client User (Data Entity)**
    *   Represents scraped LinkedIn profiles stored in the backend database (`User` model, role: CLIENT).
    *   Not an interactive user.

### Key User Flows (via Consuming Application)

1.  **Campaign Creation and Execution**
    *   User defines campaign criteria in the frontend UI.
    *   Frontend sends API request to create the campaign.
    *   Backend executes search/scraping based on schedule or trigger.
    *   Frontend polls API or receives notification (if implemented) on completion.
    *   User reviews leads retrieved via API calls in the frontend UI.

2.  **Lead Management**
    *   User views leads (fetched via API) in the frontend UI.
    *   User updates status/assignment via UI actions triggering API calls.
    *   User adds/views comments via API-driven UI components.

## Technical Architecture

### System Components

1.  **Backend API Server**
    *   Node.js/Express.js framework (planned migration to TypeScript).
    *   RESTful API endpoints for all features.
    *   JWT-based authentication middleware.
    *   Centralized error handling.
    *   **Explicitly excludes UI components.**

2.  **Database Layer**
    *   MongoDB with Mongoose ODM.
    *   Utilizes Mongoose's built-in connection pooling.
    *   Structured data models for all entities.
    *   Schema validation.

3.  **Scraping Engine**
    *   Selenium WebDriver with platform-specific `chromedriver`.
    *   Modular architecture for search and profile scraping tasks.
    *   Robust error handling and recovery mechanisms.
    *   Configurable behavior (headless/non-headless, proxy usage).

4.  **Scheduling System**
    *   `node-schedule` for cron-based execution of campaigns.
    *   Redis for distributed locking (`isFree` key) to prevent concurrent scraping runs.
    *   Configurable scheduling parameters (via environment variables).

### Data Models

1.  **User Model**
    *   Distinguishes App Users (Admin/User) from Scraped Profiles (Client).
    *   Stores authentication details (hashed passwords for App Users), role, profile info, scraped data (for Clients), activity metrics, rating.

2.  **LinkedIn Account Model**
    *   Stores credentials. **Requires encryption-at-rest for passwords.**
    *   Usage statistics and status.

3.  **Proxy Model**
    *   Stores proxy server details (`value` field, including auth if needed).
    *   Usage tracking.

4.  **Campaign Model**
    *   Search parameters, filters, linked account/proxy IDs.
    *   Execution status, flags (`isSearched`, `processing`), results (`totalResults`, `resultsArr` referencing `Lead` model).
    *   **Avoid storing plain text passwords.**

5.  **Lead Model**
    *   Acts as a link: `campaignId` -> `clientId` (ref to scraped User profile) -> `leadAssignedToId` (ref to App User).
    *   Tracks status, rating.

6.  **Supporting Models**
    *   `PreviousLeads`: For de-duplication.
    *   `LeadStatus`: Definitions.
    *   `LeadComment`: User comments.
    *   `LeadLogs`: Activity logs.
    *   `EmailSettings`: SMTP configuration.
    *   `UserLogs`: (Purpose needs clarification, potentially app user actions).

### APIs and Integrations

1.  **Authentication API**
    *   `/users/login`, `/users/register`, `/users/loginAdmin`, `/users/registerAdmin`.
    *   Token management implicitly handled via JWT responses.

2.  **User Management API**
    *   CRUD for users: `/users/getUsers`, `/users/getById/:id`, `/users/updateById/:id`, `/users/deleteById/:id`.
    *   `/users/setUserRating` (internal trigger).

3.  **Campaign Management API**
    *   CRUD: `/campaign/addCampaign`, `/campaign/getCampaigns`, `/campaign/getById/:id`, `/campaign/updateById/:id`, `/campaign/deleteById/:id`.
    *   Execution/Control: `/campaign/addCampaignToQueue`, `/campaign/searchLinkedin`, `/campaign/linkedInProfileScrappingReq`, `/campaign/linkedInLogin`, `/campaign/checkLinkedInLogin`, `/campaign/getLinkedInCaptcha`, `/campaign/sendLinkedInCaptchaInput`, `/campaign/verifyOtp`, `/campaign/resendPhoneCheck`, `/campaign/forceCloseDriver`.
    *   Reporting: `/campaign/getPastCampaign`, `/campaign/getPastCampaignById/:id`, `/campaign/sendCampaignToSevanta`.
    *   Scheduling: `/campaign/addScheduledCampaign`.

4.  **Lead Management API**
    *   CRUD: `/lead/getLeads`, `/lead/getById/:id`, `/lead/updateById/:id`, `/lead/deleteById/:id`.
    *   Export: `/lead/exportLeadsToExcel`.

5.  **Configuration APIs**
    *   `/linkedInAccount/...` (CRUD)
    *   `/proxies/...` (CRUD)
    *   `/emailSettings/...` (CRUD)
    *   `/leadStatus/...` (CRUD)

6.  **Annotation APIs**
    *   `/leadComments/...` (CRUD)
    *   `/leadlogs/getLeadLogs` (Read)

7.  **External Integrations**
    *   LinkedIn.com (via Selenium)
    *   Email Service Provider (via Nodemailer)
    *   MongoDB Database
    *   Redis Server

### Infrastructure Requirements

1.  **Runtime Environment**
    *   Node.js (TypeScript planned)
    *   Redis instance
    *   MongoDB instance

2.  **Dependencies**
    *   Selenium WebDriver + platform-specific `chromedriver`
    *   `mongoose`
    *   `express`
    *   `jsonwebtoken`, `bcryptjs`
    *   `node-schedule`
    *   `redis` client
    *   `nodemailer`
    *   Other utilities...

3.  **Development Tools**
    *   `@babel/node` (current) / TypeScript compiler (future)
    *   ESLint / Prettier
    *   Testing framework (Jest recommended)
    *   Docker (optional, for containerization)

## Development Roadmap

### Phase 1: Foundation and Core Infrastructure

1. **Project Setup**
   - TypeScript configuration
   - Project structure
   - Basic Express server
   - MongoDB connection

2. **Authentication System**
   - User model implementation
   - JWT authentication
   - Password encryption
   - Role-based authorization

3. **Configuration Management**
   - Environment variables
   - Configuration service
   - Database initialization

4. **Error Handling**
   - Global error middleware
   - Specialized error types
   - Consistent error responses

### Phase 2: Resource Management

1. **LinkedIn Account Management**
   - Model implementation
   - API endpoints for CRUD
   - **Implement secure credential storage (encryption)**

2. **Proxy Management**
   - Model implementation
   - API endpoints for CRUD
   - Proxy validation (basic format check)

3. **Email Configuration**
   - Email settings model
   - API endpoints for CRUD
   - Integration with Nodemailer helper

### Phase 3: Campaign and Scraping Core

1. **Campaign Management**
   - Model implementation
   - CRUD operations
   - Campaign status tracking

2. **Selenium Integration**
   - WebDriver setup
   - Proxy integration
   - Headless browser configuration

3. **LinkedIn Authentication**
   - Login flow implementation
   - CAPTCHA and OTP handling
   - Session management

### Phase 4: Scraping Functionality

1. **Search Implementation**
   - LinkedIn search automation
   - Filter application
   - Results pagination
   - Profile URL extraction

2. **Profile Scraping**
   - Profile data extraction
   - De-duplication logic
   - Data normalization

3. **Redis Integration**
   - Distributed locking
   - Concurrency control
   - State management

### Phase 5: Lead Management

1. **Lead Model Implementation**
   - Data structure design (as link table)
   - API endpoints for CRUD
   - Relationship management (population)

2. **Lead Status System**
   - API endpoints for custom status definitions
   - Logic for status transitions via Lead API
   - Filtering/sorting by status via Lead API

3. **Lead Comments and Logs**
   - API endpoints for comment CRUD
   - API endpoint for retrieving logs
   - Automated log creation during key events

### Phase 6: Scheduling and Automation

1. **Cron System**
   - Scheduled job implementation
   - Campaign execution automation
   - Error recovery

2. **Alert System**
   - Email notifications
   - System monitoring
   - Error reporting

### Phase 7: API Enhancements and Optimization

1. **API Refinement**
   - Response standardization
   - Query optimization
   - Pagination implementation

2. **Performance Optimization**
   - Database indexing
   - Caching strategies
   - Rate limiting

3. **Final Testing and Documentation**
   - API documentation
   - Testing coverage
   - Deployment documentation

## Logical Dependency Chain

The development should follow this logical progression:

1. **Foundation First**
   - TypeScript project setup
   - Basic Express server
   - MongoDB connection
   - Authentication system

2. **Resource Management**
   - LinkedIn account and proxy management must be implemented before scraping
   - Configuration systems needed before dependent features

3. **Core Features in Order**
   - Campaign management (depends on authentication and resource management)
   - Selenium integration (depends on proxy management)
   - Search implementation (depends on campaign management and Selenium)
   - Profile scraping (depends on search implementation)

4. **Enhancement Features**
   - Lead management (depends on profile scraping)
   - Scheduling (depends on all scraping functionality)
   - Comments and logs (depends on lead management)

5. **Optimization and Refinement**
   - API enhancements
   - Performance optimization
   - Documentation

## Risks and Mitigations

### Technical Challenges

1. **LinkedIn Detection Avoidance**
   - **Risk**: LinkedIn may detect and block automated scraping
   - **Mitigation**: Implement random delays, proxy rotation, browser fingerprint masking

2. **Selenium Stability**
   - **Risk**: Selenium automation may break with LinkedIn UI changes
   - **Mitigation**: Implement robust error handling, selectors with fallbacks, regular monitoring

3. **Data Volume Management**
   - **Risk**: Large amounts of scraped data may impact performance
   - **Mitigation**: Implement pagination, optimize database queries, consider data archiving

### Resource Constraints

1. **Processing Requirements**
   - **Risk**: Selenium automation requires significant CPU/memory
   - **Mitigation**: Optimize Selenium usage, implement resource limits, consider containerization

2. **Concurrent Execution**
   - **Risk**: Multiple concurrent scraping sessions may lead to instability
   - **Mitigation**: Implement robust locking with Redis, queue system for campaigns

### Security Concerns

1. **Credential Storage**
   - **Risk**: Storing LinkedIn/Proxy/SMTP credentials insecurely (plain text identified).
   - **Mitigation**: **Mandatory implementation of encryption-at-rest** for all sensitive credentials stored in the database. Enforce strict admin-only access controls on management APIs.

2. **API Security**
   - **Risk**: Unauthorized access to API endpoints.
   - **Mitigation**: Implement robust role-based access control middleware verifying JWTs for all sensitive APIs. Implement rate limiting on public/auth endpoints.
   - **Risk**: Input validation gaps.
   - **Mitigation**: Implement thorough input validation on all API request bodies and parameters.

## Appendix

### Technology Stack

- **Language**: Node.js (using Babel), **TypeScript migration planned**
- **Framework**: Express.js
- **Database**: MongoDB with Mongoose
- **Caching/Locking**: Redis
- **Automation**: Selenium WebDriver with platform-specific Chrome Driver
- **Authentication**: JWT, bcrypt
- **Scheduling**: node-schedule
- **Email**: Nodemailer
- **Testing**: Jest
- **Linting**: ESLint, Prettier

### LinkedIn Scraping Technical Specifications

- WebDriver initialization with proxy support and platform detection.
- Session handling and cookie management
- XPath/CSS selectors for LinkedIn elements
- Rate limiting and anti-detection measures
- Error recovery procedures